{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Training\n",
    "\n",
    "\n",
    "### Code is adapted from https://github.com/AI4Finance-Foundation/FinRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import finrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xt1317y2ixSS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl import config_tickers\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>14.621429</td>\n",
       "      <td>14.732143</td>\n",
       "      <td>14.607143</td>\n",
       "      <td>12.500195</td>\n",
       "      <td>302220800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.628789</td>\n",
       "      <td>12.438779</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12.500195</td>\n",
       "      <td>12.500195</td>\n",
       "      <td>22.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>64.949997</td>\n",
       "      <td>65.190002</td>\n",
       "      <td>63.450001</td>\n",
       "      <td>48.132034</td>\n",
       "      <td>10216800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.628789</td>\n",
       "      <td>12.438779</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>48.132034</td>\n",
       "      <td>48.132034</td>\n",
       "      <td>22.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>AXP</td>\n",
       "      <td>48.299999</td>\n",
       "      <td>48.959999</td>\n",
       "      <td>48.139999</td>\n",
       "      <td>41.038830</td>\n",
       "      <td>6955400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.628789</td>\n",
       "      <td>12.438779</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>41.038830</td>\n",
       "      <td>41.038830</td>\n",
       "      <td>22.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>BA</td>\n",
       "      <td>74.699997</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>74.120003</td>\n",
       "      <td>60.731083</td>\n",
       "      <td>6859300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.628789</td>\n",
       "      <td>12.438779</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.731083</td>\n",
       "      <td>60.731083</td>\n",
       "      <td>22.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>CAT</td>\n",
       "      <td>92.769997</td>\n",
       "      <td>95.110001</td>\n",
       "      <td>92.769997</td>\n",
       "      <td>69.014999</td>\n",
       "      <td>8177000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.628789</td>\n",
       "      <td>12.438779</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>69.014999</td>\n",
       "      <td>69.014999</td>\n",
       "      <td>22.969999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   tic       open       high        low      close       volume  \\\n",
       "                                                                               \n",
       "0  2012-01-03  AAPL  14.621429  14.732143  14.607143  12.500195  302220800.0   \n",
       "0  2012-01-03  AMGN  64.949997  65.190002  63.450001  48.132034   10216800.0   \n",
       "0  2012-01-03   AXP  48.299999  48.959999  48.139999  41.038830    6955400.0   \n",
       "0  2012-01-03    BA  74.699997  75.000000  74.120003  60.731083    6859300.0   \n",
       "0  2012-01-03   CAT  92.769997  95.110001  92.769997  69.014999    8177000.0   \n",
       "\n",
       "   day  macd    boll_ub    boll_lb  rsi_30     cci_30  dx_30  close_30_sma  \\\n",
       "                                                                             \n",
       "0  1.0   0.0  12.628789  12.438779   100.0  66.666667  100.0     12.500195   \n",
       "0  1.0   0.0  12.628789  12.438779   100.0  66.666667  100.0     48.132034   \n",
       "0  1.0   0.0  12.628789  12.438779   100.0  66.666667  100.0     41.038830   \n",
       "0  1.0   0.0  12.628789  12.438779   100.0  66.666667  100.0     60.731083   \n",
       "0  1.0   0.0  12.628789  12.438779   100.0  66.666667  100.0     69.014999   \n",
       "\n",
       "   close_60_sma        vix  turbulence  \n",
       "                                        \n",
       "0     12.500195  22.969999         0.0  \n",
       "0     48.132034  22.969999         0.0  \n",
       "0     41.038830  22.969999         0.0  \n",
       "0     60.731083  22.969999         0.0  \n",
       "0     69.014999  22.969999         0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(INDICATORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State space dimension is calculated as current price + turbulence_boolean * stock_dimension + stock_dimension * indicators_per_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension #transaction fees per trade\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 100000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = True\n",
    "if_using_td3 = False\n",
    "if_using_sac = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC) to choose from. We only use PPO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "# if if_using_a2c:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/a2c'\n",
    "#   new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [],
   "source": [
    "# trained_a2c = agent.train_model(model=model_a2c, \n",
    "#                              tb_log_name='a2c',\n",
    "#                              total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "# trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "# if if_using_ddpg:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/ddpg'\n",
    "#   new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "# trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "#                              tb_log_name='ddpg',\n",
    "#                              total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "# trained_ddpg.save(TRAINED_MODEL_DIR + \"agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 182         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 11          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.06675434 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 180         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021950923 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -2.69       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.483      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.054      |\n",
      "|    reward               | 0.013629383 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0935      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.02405791   |\n",
      "|    clip_fraction        | 0.253        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -0.787       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.368       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0488      |\n",
      "|    reward               | 0.0024933266 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.04         |\n",
      "------------------------------------------\n",
      "day: 2640, episode: 80\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 29101.85\n",
      "total_reward: -70898.15\n",
      "total_cost: 60296.04\n",
      "total_trades: 47772\n",
      "Sharpe: -0.252\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.029081963  |\n",
      "|    clip_fraction        | 0.289        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -0.261       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.388       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0467      |\n",
      "|    reward               | -0.006625443 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0293       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02660966 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.3      |\n",
      "|    explained_variance   | 0.183      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.441     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0352    |\n",
      "|    reward               | 0.01600694 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 0.0331     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028928354 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -2.59       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.467      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    reward               | 0.043177206 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00647     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0356902   |\n",
      "|    clip_fraction        | 0.353       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.461      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    reward               | 0.001151432 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0123      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 183           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 89            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.030927025   |\n",
      "|    clip_fraction        | 0.329         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -41.5         |\n",
      "|    explained_variance   | 0.485         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.464        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.034        |\n",
      "|    reward               | -0.0031148905 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.0144        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.034514353  |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.6        |\n",
      "|    explained_variance   | 0.614        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.465       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0341      |\n",
      "|    reward               | -0.025776241 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00871      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.03693552   |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.7        |\n",
      "|    explained_variance   | -0.0813      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.453       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0299      |\n",
      "|    reward               | 0.0071952716 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00614      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037597537 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.444      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.036      |\n",
      "|    reward               | 0.008170931 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0153      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038102537 |\n",
      "|    clip_fraction        | 0.372       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.452      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    reward               | 0.021529255 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.0171      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.034366883  |\n",
      "|    clip_fraction        | 0.322        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42          |\n",
      "|    explained_variance   | 0.75         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.485       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0326      |\n",
      "|    reward               | -0.044694833 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.00766      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035594005 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.476      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    reward               | 0.009291341 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03998203  |\n",
      "|    clip_fraction        | 0.4         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | -0.594      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.453      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    reward               | 0.004609826 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.00793     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037533775 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.474      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0344     |\n",
      "|    reward               | -0.00572775 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.00818     |\n",
      "-----------------------------------------\n",
      "day: 2640, episode: 90\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 49332.57\n",
      "total_reward: -50667.43\n",
      "total_cost: 59131.18\n",
      "total_trades: 47108\n",
      "Sharpe: -0.034\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.036143698  |\n",
      "|    clip_fraction        | 0.353        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.3        |\n",
      "|    explained_variance   | 0.743        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.472       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0396      |\n",
      "|    reward               | -0.018172253 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.00634      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 200         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036605258 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.442      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    reward               | 0.02067464  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0198      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.04265006  |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.065       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.457      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    reward               | 0.017790113 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0162      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042321518 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.464      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0346     |\n",
      "|    reward               | 0.011957028 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 233          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.051107742  |\n",
      "|    clip_fraction        | 0.405        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | 0.621        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.489       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.0242      |\n",
      "|    reward               | -0.023985956 |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.0085       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 245          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.040355958  |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.6        |\n",
      "|    explained_variance   | 0.54         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.437       |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    reward               | -0.018212724 |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.0175       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 183          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 257          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.04615632   |\n",
      "|    clip_fraction        | 0.387        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.7        |\n",
      "|    explained_variance   | 0.559        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.478       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0202      |\n",
      "|    reward               | 0.0088523505 |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 0.018        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.05165457  |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | -0.0787     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.449      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    reward               | 0.018865608 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 279         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051360115 |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.45       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    reward               | 0.009454348 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.00964     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040393963 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.446      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    reward               | 0.008870432 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 0.00804     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 303          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0515282    |\n",
      "|    clip_fraction        | 0.352        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.1        |\n",
      "|    explained_variance   | 0.871        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.461       |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0234      |\n",
      "|    reward               | 0.0099453535 |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 0.00376      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0488226   |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | -0.0213     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.469      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    reward               | 0.019246811 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 325         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054037154 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.478      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    reward               | 0.008828753 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 0.00746     |\n",
      "-----------------------------------------\n",
      "day: 2640, episode: 100\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 33058.80\n",
      "total_reward: -66941.20\n",
      "total_cost: 29982.34\n",
      "total_trades: 44439\n",
      "Sharpe: -0.025\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 336          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.03841173   |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.3        |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.456       |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.0202      |\n",
      "|    reward               | -0.010253674 |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 0.0157       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 347          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.043563336  |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.3        |\n",
      "|    explained_variance   | 0.761        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.485       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0224      |\n",
      "|    reward               | -0.033406667 |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 0.00938      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 360          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.057918523  |\n",
      "|    clip_fraction        | 0.425        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.4        |\n",
      "|    explained_variance   | 0.679        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.472       |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    reward               | -0.033930883 |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 0.0119       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 371           |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.04244864    |\n",
      "|    clip_fraction        | 0.357         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -43.4         |\n",
      "|    explained_variance   | 0.0636        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.459        |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.0231       |\n",
      "|    reward               | -0.0034589146 |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 0.0109        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 382         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.04172333  |\n",
      "|    clip_fraction        | 0.375       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.455      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    reward               | 0.001103677 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 0.0122      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 181          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 393          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.03806278   |\n",
      "|    clip_fraction        | 0.367        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.6        |\n",
      "|    explained_variance   | 0.697        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.47        |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.0169      |\n",
      "|    reward               | -0.029418629 |\n",
      "|    std                  | 1.09         |\n",
      "|    value_loss           | 0.0155       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 404          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.037868794  |\n",
      "|    clip_fraction        | 0.366        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.7        |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.459       |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0209      |\n",
      "|    reward               | -0.012588765 |\n",
      "|    std                  | 1.09         |\n",
      "|    value_loss           | 0.00432      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 416         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038691755 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.0868      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.463      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    reward               | 0.035921022 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 0.0208      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 427         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045774214 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.468      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    reward               | 0.008724349 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 0.0129      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 438           |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.04329227    |\n",
      "|    clip_fraction        | 0.365         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -43.9         |\n",
      "|    explained_variance   | 0.684         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.474        |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.0237       |\n",
      "|    reward               | -0.0065008365 |\n",
      "|    std                  | 1.1           |\n",
      "|    value_loss           | 0.0169        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 449           |\n",
      "|    total_timesteps      | 81920         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.04980783    |\n",
      "|    clip_fraction        | 0.348         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -44           |\n",
      "|    explained_variance   | 0.714         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.448        |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.0178       |\n",
      "|    reward               | -0.0017852595 |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 0.0153        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 460         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042012762 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.475      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    reward               | 0.014417617 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.04331732  |\n",
      "|    clip_fraction        | 0.362       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.501      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    reward               | 0.003430348 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.00525     |\n",
      "-----------------------------------------\n",
      "day: 2640, episode: 110\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 24103.20\n",
      "total_reward: -75896.80\n",
      "total_cost: 22469.26\n",
      "total_trades: 44131\n",
      "Sharpe: -0.096\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 483         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.04586529  |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.475      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    reward               | -0.15487495 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 0.00575     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 494         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054008164 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | -0.143      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.451      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    reward               | 0.00651204  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 0.0202      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 505          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.04055191   |\n",
      "|    clip_fraction        | 0.343        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.4        |\n",
      "|    explained_variance   | 0.3          |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.409       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    reward               | 0.0021670896 |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 0.0752       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 516          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.056845453  |\n",
      "|    clip_fraction        | 0.359        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.5        |\n",
      "|    explained_variance   | -1.27        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.478       |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    reward               | -0.025939558 |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 0.00446      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 527         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041710902 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.487      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    reward               | 0.029291186 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 0.00588     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 538          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.040252127  |\n",
      "|    clip_fraction        | 0.332        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.7        |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.485       |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.0217      |\n",
      "|    reward               | -0.019128906 |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 0.00657      |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 550        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0468025  |\n",
      "|    clip_fraction        | 0.347      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.8      |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.473     |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    reward               | 0.11284629 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 0.00753    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 561         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.05876745  |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.431      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | 0.023546843 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 0.0105      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 572         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030074667 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.488      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    reward               | 0.024516212 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 0.0112      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 583         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043498747 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.0901      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.494      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | 0.021984922 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 0.0326      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 53            |\n",
      "|    time_elapsed         | 595           |\n",
      "|    total_timesteps      | 108544        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.06560222    |\n",
      "|    clip_fraction        | 0.428         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -45.1         |\n",
      "|    explained_variance   | 0.447         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.427        |\n",
      "|    n_updates            | 520           |\n",
      "|    policy_gradient_loss | -0.00391      |\n",
      "|    reward               | -0.0062596155 |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 0.0509        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 606          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.03310862   |\n",
      "|    clip_fraction        | 0.354        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.2        |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.463       |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.0118      |\n",
      "|    reward               | -0.006200064 |\n",
      "|    std                  | 1.15         |\n",
      "|    value_loss           | 0.0233       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 617         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046599165 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.461      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    reward               | 0.3068085   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.0451      |\n",
      "-----------------------------------------\n",
      "day: 2640, episode: 120\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 145389.95\n",
      "total_reward: 45389.95\n",
      "total_cost: 31928.04\n",
      "total_trades: 45377\n",
      "Sharpe: 0.286\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 629         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.04340635  |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.45       |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    reward               | 0.008966844 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.0835      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050262697 |\n",
      "|    clip_fraction        | 0.368       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.441      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | 0.023437914 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 58            |\n",
      "|    time_elapsed         | 651           |\n",
      "|    total_timesteps      | 118784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.057667337   |\n",
      "|    clip_fraction        | 0.355         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -45.6         |\n",
      "|    explained_variance   | 0.593         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.435        |\n",
      "|    n_updates            | 570           |\n",
      "|    policy_gradient_loss | -0.0133       |\n",
      "|    reward               | -0.0077545284 |\n",
      "|    std                  | 1.17          |\n",
      "|    value_loss           | 0.0228        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 662         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043516017 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.389       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.468      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    reward               | 0.007374352 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.0318      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 673           |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.04187035    |\n",
      "|    clip_fraction        | 0.369         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -45.8         |\n",
      "|    explained_variance   | 0.769         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.486        |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -0.0165       |\n",
      "|    reward               | -0.0067074792 |\n",
      "|    std                  | 1.18          |\n",
      "|    value_loss           | 0.0104        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 684         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055185962 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.469      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    reward               | 0.04797442  |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 0.0162      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 62            |\n",
      "|    time_elapsed         | 695           |\n",
      "|    total_timesteps      | 126976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.038590707   |\n",
      "|    clip_fraction        | 0.326         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -46           |\n",
      "|    explained_variance   | 0.478         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.434        |\n",
      "|    n_updates            | 610           |\n",
      "|    policy_gradient_loss | -0.0159       |\n",
      "|    reward               | -0.0020418356 |\n",
      "|    std                  | 1.19          |\n",
      "|    value_loss           | 0.0686        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 707          |\n",
      "|    total_timesteps      | 129024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.049966488  |\n",
      "|    clip_fraction        | 0.355        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.1        |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.493       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00719     |\n",
      "|    reward               | -0.022370994 |\n",
      "|    std                  | 1.19         |\n",
      "|    value_loss           | 0.0191       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 718          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.049343828  |\n",
      "|    clip_fraction        | 0.392        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.1        |\n",
      "|    explained_variance   | 0.38         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.473       |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00751     |\n",
      "|    reward               | -0.022412686 |\n",
      "|    std                  | 1.19         |\n",
      "|    value_loss           | 0.00807      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 729         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046611816 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.2       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.463      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    reward               | 0.017035084 |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 0.0158      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 740          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.054566152  |\n",
      "|    clip_fraction        | 0.372        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.3        |\n",
      "|    explained_variance   | 0.676        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.458       |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    reward               | 0.0065807006 |\n",
      "|    std                  | 1.19         |\n",
      "|    value_loss           | 0.0286       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 751          |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.05717552   |\n",
      "|    clip_fraction        | 0.346        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.3        |\n",
      "|    explained_variance   | 0.797        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.491       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0181      |\n",
      "|    reward               | -0.023451034 |\n",
      "|    std                  | 1.2          |\n",
      "|    value_loss           | 0.0135       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 762          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.08382186   |\n",
      "|    clip_fraction        | 0.384        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.4        |\n",
      "|    explained_variance   | 0.535        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.448       |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00751     |\n",
      "|    reward               | -0.030488389 |\n",
      "|    std                  | 1.2          |\n",
      "|    value_loss           | 0.0156       |\n",
      "------------------------------------------\n",
      "day: 2640, episode: 130\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 58910.72\n",
      "total_reward: -41089.28\n",
      "total_cost: 23732.25\n",
      "total_trades: 43798\n",
      "Sharpe: -0.003\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 774         |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048217162 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.495      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    reward               | 0.003400894 |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 0.0219      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 786         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0448004   |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.492      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00618    |\n",
      "|    reward               | -0.04269473 |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 0.0184      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 797          |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.040472634  |\n",
      "|    clip_fraction        | 0.32         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.6        |\n",
      "|    explained_variance   | 0.639        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.501       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.018       |\n",
      "|    reward               | -0.009086245 |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 0.0459       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 808        |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03908374 |\n",
      "|    clip_fraction        | 0.306      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.7      |\n",
      "|    explained_variance   | 0.736      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.5       |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.00321   |\n",
      "|    reward               | 0.13608235 |\n",
      "|    std                  | 1.21       |\n",
      "|    value_loss           | 0.0394     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 819          |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.05375178   |\n",
      "|    clip_fraction        | 0.333        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.8        |\n",
      "|    explained_variance   | 0.473        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.464       |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.0166      |\n",
      "|    reward               | -0.008853853 |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 0.015        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 830        |\n",
      "|    total_timesteps      | 151552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05302076 |\n",
      "|    clip_fraction        | 0.357      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.8      |\n",
      "|    explained_variance   | 0.761      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.514     |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    reward               | -0.0652466 |\n",
      "|    std                  | 1.22       |\n",
      "|    value_loss           | 0.02       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 841         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054222174 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.9       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.496      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    reward               | 0.05017341  |\n",
      "|    std                  | 1.22        |\n",
      "|    value_loss           | 0.0191      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 853          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.046496123  |\n",
      "|    clip_fraction        | 0.319        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47          |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.471       |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    reward               | -0.035375714 |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 0.022        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 864         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033729192 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47         |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.492      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    reward               | 0.020711707 |\n",
      "|    std                  | 1.23        |\n",
      "|    value_loss           | 0.0192      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 875        |\n",
      "|    total_timesteps      | 159744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04240618 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -47.1      |\n",
      "|    explained_variance   | 0.871      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.507     |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    reward               | 0.08575305 |\n",
      "|    std                  | 1.23       |\n",
      "|    value_loss           | 0.00906    |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 886          |\n",
      "|    total_timesteps      | 161792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.051099226  |\n",
      "|    clip_fraction        | 0.282        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.2        |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.452       |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.0158      |\n",
      "|    reward               | 0.0055787307 |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 0.0199       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 897         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03886171  |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.3       |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.475      |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    reward               | 0.040836956 |\n",
      "|    std                  | 1.24        |\n",
      "|    value_loss           | 0.0292      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 908        |\n",
      "|    total_timesteps      | 165888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04631654 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -47.3      |\n",
      "|    explained_variance   | 0.619      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.486     |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    reward               | 0.03741802 |\n",
      "|    std                  | 1.24       |\n",
      "|    value_loss           | 0.05       |\n",
      "----------------------------------------\n",
      "day: 2640, episode: 140\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 88798.37\n",
      "total_reward: -11201.63\n",
      "total_cost: 22728.72\n",
      "total_trades: 44029\n",
      "Sharpe: 0.193\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 920         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034612946 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.4       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.506      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    reward               | -0.04235532 |\n",
      "|    std                  | 1.24        |\n",
      "|    value_loss           | 0.0184      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 931          |\n",
      "|    total_timesteps      | 169984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.04349055   |\n",
      "|    clip_fraction        | 0.318        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.4        |\n",
      "|    explained_variance   | 0.785        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.492       |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00901     |\n",
      "|    reward               | -0.023317184 |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 0.0193       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 942         |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053200338 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.4       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.49       |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | -0.07916015 |\n",
      "|    std                  | 1.24        |\n",
      "|    value_loss           | 0.0148      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 953         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046108045 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.5       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.471      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | 0.019330572 |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 0.0153      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 964          |\n",
      "|    total_timesteps      | 176128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.04015839   |\n",
      "|    clip_fraction        | 0.308        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.6        |\n",
      "|    explained_variance   | 0.58         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.507       |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.0112      |\n",
      "|    reward               | -0.030609427 |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 0.018        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 976          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.036995884  |\n",
      "|    clip_fraction        | 0.302        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.7        |\n",
      "|    explained_variance   | 0.697        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.523       |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0193      |\n",
      "|    reward               | -0.028250389 |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 0.0189       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 987          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.044283055  |\n",
      "|    clip_fraction        | 0.274        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.8        |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.484       |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.0146      |\n",
      "|    reward               | 0.0025976636 |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 0.0248       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 998         |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040727966 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.9       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.483      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | -0.03779731 |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 0.0209      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 182          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 1009         |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.031862184  |\n",
      "|    clip_fraction        | 0.269        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -48          |\n",
      "|    explained_variance   | 0.72         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.487       |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.0123      |\n",
      "|    reward               | -0.050680768 |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 0.0502       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1020        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049661912 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48         |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.511      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    reward               | -0.04246008 |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 0.0279      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1031        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043684907 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48         |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.472      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | 0.10087214  |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 0.0342      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 93            |\n",
      "|    time_elapsed         | 1043          |\n",
      "|    total_timesteps      | 190464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.038449757   |\n",
      "|    clip_fraction        | 0.262         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -48.1         |\n",
      "|    explained_variance   | 0.805         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.509        |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.0157       |\n",
      "|    reward               | -0.0025458562 |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 0.0401        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1054        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045681983 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.1       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.51       |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    reward               | 0.038341764 |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 0.0407      |\n",
      "-----------------------------------------\n",
      "day: 2640, episode: 150\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 96644.91\n",
      "total_reward: -3355.09\n",
      "total_cost: 16628.62\n",
      "total_trades: 43526\n",
      "Sharpe: 0.208\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 182           |\n",
      "|    iterations           | 95            |\n",
      "|    time_elapsed         | 1065          |\n",
      "|    total_timesteps      | 194560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.04357285    |\n",
      "|    clip_fraction        | 0.291         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -48.2         |\n",
      "|    explained_variance   | 0.742         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.5          |\n",
      "|    n_updates            | 940           |\n",
      "|    policy_gradient_loss | -0.006        |\n",
      "|    reward               | -0.0057734475 |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 0.0279        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1076        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040045768 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.3       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.478      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 0.051268097 |\n",
      "|    std                  | 1.28        |\n",
      "|    value_loss           | 0.0326      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 1087       |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07434964 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -48.3      |\n",
      "|    explained_variance   | 0.842      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.472     |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.00596   |\n",
      "|    reward               | 0.05235796 |\n",
      "|    std                  | 1.28       |\n",
      "|    value_loss           | 0.0272     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 1098        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.06288001  |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -48.3       |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.482      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0084     |\n",
      "|    reward               | 0.005611361 |\n",
      "|    std                  | 1.28        |\n",
      "|    value_loss           | 0.0214      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(\"agent_ppo_djia\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# TD3_PARAMS = {\"batch_size\": 100, \n",
    "#               \"buffer_size\": 1000000, \n",
    "#               \"learning_rate\": 0.001}\n",
    "\n",
    "# model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "# if if_using_td3:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/td3'\n",
    "#   new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "# trained_td3 = agent.train_model(model=model_td3, \n",
    "#                              tb_log_name='td3',\n",
    "#                              total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "# trained_td3.save(TRAINED_MODEL_DIR + \"agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# SAC_PARAMS = {\n",
    "#     \"batch_size\": 128,\n",
    "#     \"buffer_size\": 100000,\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"learning_starts\": 100,\n",
    "#     \"ent_coef\": \"auto_0.1\",\n",
    "# }\n",
    "\n",
    "# model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "# if if_using_sac:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/sac'\n",
    "#   new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "# trained_sac = agent.train_model(model=model_sac, \n",
    "#                              tb_log_name='sac',\n",
    "#                              total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "# trained_sac.save(TRAINED_MODEL_DIR + \"agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
